{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.24.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.8.0 (from gradio)\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (1.26.0)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (11.0.0)\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio) (4.12.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from gradio-client==1.8.0->gradio) (2024.6.1)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.8.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: certifi in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.33.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from python-dateutil>=2.8.1->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.24.0-py3-none-any.whl (46.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading orjson-3.10.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Installing collected packages: pydub, websockets, typing-inspection, tomlkit, sniffio, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, h11, groovy, ffmpy, click, annotated-types, aiofiles, uvicorn, pydantic, huggingface-hub, httpcore, anyio, typer, starlette, httpx, safehttpx, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.26.5\n",
      "    Uninstalling huggingface-hub-0.26.5:\n",
      "      Successfully uninstalled huggingface-hub-0.26.5\n",
      "Successfully installed aiofiles-24.1.0 annotated-types-0.7.0 anyio-4.9.0 click-8.1.8 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.24.0 gradio-client-1.8.0 groovy-0.1.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.30.2 orjson-3.10.16 pydantic-2.11.3 pydantic-core-2.33.1 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.46.1 tomlkit-0.13.2 typer-0.15.2 typing-inspection-0.4.0 uvicorn-0.34.0 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "def find_matching_column(columns: List[str], patterns: List[str]) -> str:\n",
    "    \"\"\"Auto select columns from a dataframe based on patterns.\"\"\"\n",
    "    cols_lower = [col.lower() for col in columns]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        for col in cols_lower:\n",
    "            if re.search(pattern.lower(), col):\n",
    "                return col\n",
    "    return columns[0] if columns else None\n",
    "\n",
    "find_matching_column([\"a\",\"sequencia\", \"C\"], [\"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/miniconda3/envs/protein_engineering/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m2025-04-11 17:55:56,098 - __main__ - INFO - Using device: cuda:0 (2446207300.py:136)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:55:56,099 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Using device: cuda:0 (ProteinEmbedder.py:58)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:55:56,758 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Loading model and tokenizer for Rostlab/prot_t5_xl_uniref50 (ProteinEmbedder.py:62)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:55:56,758 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Model architecture: T5ForConditionalGeneration (ProteinEmbedder.py:63)\u001b[39m\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "\u001b[37m2025-04-11 17:55:59,922 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Model and tokenizer for Rostlab/prot_t5_xl_uniref50 loaded successfully. (ProteinEmbedder.py:88)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:55:59,924 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Starting embedding process (ProteinEmbedder.py:159)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:55:59,924 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Batch size: 10 (ProteinEmbedder.py:160)\u001b[39m\n",
      "Embedding progress: 100%|██████████| 5/5 [00:00<00:00, 16.67it/s]\n",
      "\u001b[37m2025-04-11 17:56:00,227 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Embedding process finished (ProteinEmbedder.py:185)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:56:00,228 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Cleaning GPU memory (ProteinEmbedder.py:95)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:56:00,471 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Embeddings stored in memory: (50, 1024) (ProteinEmbedder.py:191)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:56:00,471 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - Embeddings: (ProteinEmbedder.py:197)\u001b[39m\n",
      "\u001b[37m2025-04-11 17:56:00,472 - ProteinEmbedder_Rostlab/prot_t5_xl_uniref50 - INFO - [[-0.0064014  -0.11859944 -0.0301141  ... -0.04372606 -0.02484127\n",
      "   0.002768  ]\n",
      " [-0.0064014  -0.11859944 -0.0301141  ... -0.04372606 -0.02484127\n",
      "   0.002768  ]\n",
      " [-0.0064014  -0.11859944 -0.0301141  ... -0.04372606 -0.02484127\n",
      "   0.002768  ]\n",
      " ...\n",
      " [-0.0064014  -0.11859944 -0.0301141  ... -0.04372606 -0.02484127\n",
      "   0.002768  ]\n",
      " [-0.0064014  -0.11859944 -0.0301141  ... -0.04372606 -0.02484127\n",
      "   0.002768  ]\n",
      " [-0.0064014  -0.11859944 -0.0301141  ... -0.04372606 -0.02484127\n",
      "   0.002768  ]] (ProteinEmbedder.py:198)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List\n",
    "from logger import get_logger\n",
    "from preprocessing.embeddings import ProteinEmbedder\n",
    "\n",
    "ENCODING_TYPES = [\n",
    "    \"FFT\",\n",
    "    \"Frequency\",\n",
    "    \"KMer\",\n",
    "    \"One-Hot\",\n",
    "    \"Ordinal\",\n",
    "    \"Physicochemical\",\n",
    "    \"Embedding\"\n",
    "]\n",
    "EMBEDDING_MODELS = {\n",
    "    \"Ankh2\": \"ElnaggarLab/ankh2-ext1\",\n",
    "    \"Bert\": \"Rostlab/prot_bert\",\n",
    "    \"ESM2\": \"facebook/esm2_t6_8M_UR50D\",\n",
    "    \"ESMC\": \"esmc_600\",\n",
    "    \"Mistral\": \"RaphaelMourad/Mistral-Prot-v1-134M\",\n",
    "    \"Prot T5\": \"Rostlab/prot_t5_xl_uniref50\"\n",
    "}\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "### Section: Interface tools ###\n",
    "\n",
    "def find_matching_column(columns: List[str], patterns: List[str]) -> str:\n",
    "    \"\"\"Auto select columns from a dataframe based on patterns.\"\"\"\n",
    "    for pattern in patterns:\n",
    "        for col in columns:\n",
    "            if re.search(pattern.lower(), col, re.IGNORECASE):\n",
    "                return col\n",
    "    return columns[0] if columns else None\n",
    "\n",
    "def load_csv(file) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(file.name)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return gr.Error(f\"Error loading file: {str(e)}\")\n",
    "    \n",
    "def get_gpu_devices():\n",
    "    gpu_devices = {}\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device_cound = torch.cuda.device_count()\n",
    "        for i in range(device_cound):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_devices.update({gpu_name: f\"cuda:{i}\"})\n",
    "    else:\n",
    "        logger.info(\"No GPU devices found.\")\n",
    "    \n",
    "    return gpu_devices\n",
    "            \n",
    "def get_properties_names():\n",
    "    \"\"\"Get the names of the properties.\"\"\"\n",
    "    df = pd.read_csv(\"../input_config/aaindex_encoders.csv\", nrows=0)\n",
    "    properties = df.columns.tolist()[1:]\n",
    "    return properties\n",
    "    \n",
    "\n",
    "def filter_records(records, gender):\n",
    "    return records[records[\"gender\"] == gender]\n",
    "\n",
    "\n",
    "### Section: Data Retrieval ###\n",
    "\n",
    "def init_data_retrieving():\n",
    "    return gr.Interface(\n",
    "        fn=filter_records,\n",
    "        inputs=[\n",
    "            gr.Dataframe(\n",
    "                headers=[\"name\", \"age\", \"gender\"],\n",
    "                datatype=[\"str\", \"number\", \"str\"],\n",
    "                row_count=5,\n",
    "                col_count=(3, \"fixed\"),\n",
    "            ),\n",
    "            gr.Dropdown([\"M\", \"F\", \"O\"]),\n",
    "        ],\n",
    "        outputs=\"dataframe\",\n",
    "        description=\"Enter gender as 'M', 'F', or 'O' for other.\",\n",
    "    )\n",
    "\n",
    "### Section: Data Preprocessing ###\n",
    "\n",
    "def use_embedding(\n",
    "        file,\n",
    "        sequence_col: str,\n",
    "        response_col: str,\n",
    "        device: str,\n",
    "        model: str,\n",
    "        max_length: int):\n",
    "    \"\"\"Use a pre-trained model to encode sequences.\"\"\"\n",
    "    try:\n",
    "        instance = ProteinEmbedder(\n",
    "            device=device, \n",
    "            model_name=model,\n",
    "            dataset=file[:50], \n",
    "            column_seq=sequence_col, \n",
    "            column_label=response_col,\n",
    "            columns_ignore=[response_col]\n",
    "        )\n",
    "        instance.loadModelTokenizer()\n",
    "        instance.embeddingProcess(batch_size=10)\n",
    "        instance.showEmbeddings()\n",
    "        return instance.getDataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in embedding process: {str(e)}\")\n",
    "        return gr.Error(f\"Error in embedding process: {str(e)}\")\n",
    "\n",
    "\n",
    "def encode_data(\n",
    "        file, \n",
    "        sequence_col: str, \n",
    "        response_col: str, \n",
    "        device: str, \n",
    "        encoding: str, \n",
    "        max_length: int, \n",
    "        name_property: str, \n",
    "        size_kmer: int, \n",
    "        embedding_model: str):\n",
    "    \"\"\"Process the input dataframe and return the result.\"\"\"\n",
    "    try:\n",
    "        df = load_csv(file)\n",
    "        \n",
    "        if sequence_col not in df.columns or response_col not in df.columns:\n",
    "            raise ValueError(\"Selected columns are not in the dataframe.\")\n",
    "        \n",
    "        sequences = df[sequence_col].tolist()\n",
    "        responses = df[response_col].tolist()\n",
    "        device = get_gpu_devices().get(device, \"cpu\")\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        model = EMBEDDING_MODELS[embedding_model]\n",
    "\n",
    "        match encoding:\n",
    "            case \"One-Hot\":\n",
    "                # Implement One-Hot Encoding\n",
    "                pass\n",
    "            case \"Ordinal\":\n",
    "                # Implement Ordinal Encoding\n",
    "                pass\n",
    "            case \"Frequency\":\n",
    "                # Implement Frequency Encoding\n",
    "                pass\n",
    "            case \"KMer\":\n",
    "                # Implement K-mer Encoding\n",
    "                pass\n",
    "            case \"FFT\":\n",
    "                # Implement FFT Encoding\n",
    "                pass\n",
    "            case \"Physicochemical\":\n",
    "                # Implement Physicochemical Encoding\n",
    "                pass\n",
    "            case \"Embedding\":\n",
    "                encoded = use_embedding(df, sequence_col, response_col, device, model, max_length)\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"Error processing data: {str(e)}\")\n",
    "    \n",
    "    return gr.update(\n",
    "        value=encoded.head(),\n",
    "        headers=encoded.columns.tolist(),\n",
    "        row_count=min(10, len(encoded)),\n",
    "        col_count=(len(encoded.columns), \"fixed\"),\n",
    "        interactive=True,\n",
    "        visible=True\n",
    "    )\n",
    "\n",
    "def update_on_upload(file):\n",
    "    \"\"\"Update dropdowns and preview based on the uploaded file.\"\"\"\n",
    "    # Load partially to get column names\n",
    "    try:\n",
    "        df = pd.read_csv(file.name, nrows=5)\n",
    "    except Exception as e:\n",
    "        return gr.Error(f\"Error loading file: {str(e)}\")\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    sequence_col = find_matching_column(columns, [\"sequence\", \"seq\"])\n",
    "    response_col = find_matching_column(columns, [\"response\", \"label\"])\n",
    "    return gr.update(\n",
    "        choices=columns,\n",
    "        value=sequence_col,\n",
    "        visible=True\n",
    "    ), gr.update(\n",
    "        choices=columns,\n",
    "        value=response_col,\n",
    "        visible=True\n",
    "    ), gr.update(\n",
    "        value=df[[sequence_col, response_col]].head(),\n",
    "        visible=True\n",
    "    )\n",
    "\n",
    "def update_encoding_parameters(encoding_type):\n",
    "    \"\"\"Update encoding parameters based on the selected encoding type.\"\"\"\n",
    "    max_length_input = gr.update(visible=False)\n",
    "    name_property_input = gr.update(visible=False)\n",
    "    size_kmer_input = gr.update(visible=False)\n",
    "    model_selector = gr.update(visible=False)\n",
    "\n",
    "    if encoding_type in [\"One-Hot\", \"Ordinal\", \"Frequency\"]:\n",
    "        max_length_input = gr.update(visible=True)\n",
    "    elif encoding_type == \"Kmer\":\n",
    "        size_kmer_input = gr.update(visible=True)\n",
    "    \n",
    "    elif encoding_type in [\"Physicochemical\", \"FFT\"]:\n",
    "        name_property_input = gr.update(\n",
    "            choices=get_properties_names(),\n",
    "            value=get_properties_names()[0],\n",
    "            visible=True\n",
    "        )\n",
    "        max_length_input = gr.update(visible=True)\n",
    "    elif encoding_type == \"Embedding\":\n",
    "        model_selector = gr.update(\n",
    "            choices=list(EMBEDDING_MODELS.keys()),\n",
    "            value=list(EMBEDDING_MODELS.keys())[0],\n",
    "            visible=True\n",
    "        )\n",
    "    \n",
    "    return max_length_input, name_property_input, size_kmer_input, model_selector\n",
    "\n",
    "    \n",
    "\n",
    "def init_preprocessing():\n",
    "    \"\"\"Initialize the data preprocessing interface.\"\"\"\n",
    "    gpu_devices = get_gpu_devices()\n",
    "\n",
    "    ## Interface ##\n",
    "\n",
    "    with gr.Blocks() as preprocessing:\n",
    "        gr.Markdown(\"## Data Preprocessing\")\n",
    "        gr.Markdown(\"Upload a CSV file to display its contents.\")\n",
    "        with gr.Row():\n",
    "            \n",
    "            file_input = gr.File(\n",
    "                label=\"Upload CSV File\", \n",
    "                file_types=[\".csv\"]\n",
    "            )\n",
    "        \n",
    "        gr.Markdown(\"### Data Preview\")\n",
    "        \n",
    "        preview = gr.Dataframe(\n",
    "            visible=False,\n",
    "            interactive=False\n",
    "        )\n",
    "\n",
    "        # TODO remove option selected in one dropdown from the other dropdown\n",
    "        with gr.Row():\n",
    "            dropdown_sequence_col = gr.Dropdown(\n",
    "                label=\"Sequence Column\",\n",
    "                interactive=True,\n",
    "                info=\"Select the column containing sequences\",\n",
    "                visible=False\n",
    "            )\n",
    "            dropdown_response_col = gr.Dropdown(\n",
    "                label=\"Response Column\",\n",
    "                interactive=True,\n",
    "                info=\"Select the column containing responses\",\n",
    "                visible=False\n",
    "            )\n",
    "        \n",
    "        with gr.Row():\n",
    "            checkbox_cuda = gr.Checkbox(\n",
    "                label=\"Use CUDA\",\n",
    "                value=True if gpu_devices else False,\n",
    "                info=\"Use GPU acceleration if available.\",\n",
    "                interactive=True if gpu_devices else False\n",
    "            )\n",
    "            device_selector = gr.Dropdown(\n",
    "                label=\"Device\",\n",
    "                choices=list(gpu_devices.keys()),\n",
    "                value=list(gpu_devices.keys())[0] if gpu_devices else \"cpu\",\n",
    "                info=\"Select the device for processing.\",\n",
    "                interactive=True\n",
    "            )\n",
    "        \n",
    "        with gr.Row():\n",
    "            encoding_selector = gr.Dropdown(\n",
    "                label=\"Select Model\",\n",
    "                choices=ENCODING_TYPES,\n",
    "                value=ENCODING_TYPES[0],\n",
    "                info=\"Select the model for processing.\",\n",
    "                interactive=True\n",
    "            )\n",
    "            max_length_input = gr.Number(\n",
    "                label=\"Max Length\",\n",
    "                value=50,\n",
    "                info=\"Maximum length for encoding.\",\n",
    "                interactive=True,\n",
    "                visible=False\n",
    "            )\n",
    "            name_property_input = gr.Dropdown(\n",
    "                label=\"Select Property\",\n",
    "                info=\"Select the property for encoding.\",\n",
    "                interactive=True,\n",
    "                visible=False\n",
    "            )\n",
    "            size_kmer_input = gr.Number(\n",
    "                label=\"K-mer Size\",\n",
    "                value=3,\n",
    "                info=\"Size of the k-mer for encoding.\",\n",
    "                interactive=True,\n",
    "                visible=False\n",
    "            )\n",
    "            model_selector = gr.Dropdown(\n",
    "                label=\"Select Model\",\n",
    "                info=\"Select the model for encoding.\",\n",
    "                interactive=True,\n",
    "                visible=False\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            process_btn = gr.Button(\"Process Data\", variant=\"primary\", interactive=True)\n",
    "                \n",
    "\n",
    "        gr.Markdown(\"### Result Preview\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            result_preview = gr.Dataframe(\n",
    "                visible=False,\n",
    "                interactive=True\n",
    "            )\n",
    "\n",
    "        ## Logic ##\n",
    "\n",
    "        file_input.upload(\n",
    "            fn=update_on_upload,\n",
    "            inputs=file_input,\n",
    "            outputs=[dropdown_sequence_col, dropdown_response_col, preview]\n",
    "        )\n",
    "\n",
    "        checkbox_cuda.change(\n",
    "            fn=lambda x: gr.update(visible=x),\n",
    "            inputs=checkbox_cuda,\n",
    "            outputs=device_selector\n",
    "        )\n",
    "\n",
    "        encoding_selector.change(\n",
    "            fn=update_encoding_parameters,\n",
    "            inputs=encoding_selector,\n",
    "            outputs=[\n",
    "                max_length_input,\n",
    "                name_property_input,\n",
    "                size_kmer_input,\n",
    "                model_selector\n",
    "            ]\n",
    "        )\n",
    "        process_btn.click(\n",
    "            fn=encode_data,\n",
    "            inputs=[\n",
    "                file_input,\n",
    "                dropdown_sequence_col,\n",
    "                dropdown_response_col,\n",
    "                device_selector,\n",
    "                encoding_selector,\n",
    "                max_length_input,\n",
    "                name_property_input,\n",
    "                size_kmer_input,\n",
    "                model_selector\n",
    "            ],\n",
    "            outputs=[\n",
    "                result_preview\n",
    "            ]\n",
    "        )\n",
    "       \n",
    "    return preprocessing\n",
    "        \n",
    "        \n",
    "### Section: Machine Learning Tools ###\n",
    "\n",
    "def init_ml_tools():\n",
    "    return gr.Interface(\n",
    "        fn=lambda x: x,\n",
    "        inputs=\"text\",\n",
    "        outputs=\"text\",\n",
    "        title=\"Machine Learning Tools\",\n",
    "        description=\"This is a demo for machine learning tools.\"\n",
    "    )\n",
    "\n",
    "\n",
    "### Section: Main Function ###\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_retrieving = init_data_retrieving()\n",
    "    preprocessing = init_preprocessing()\n",
    "    ml_tools = init_ml_tools()\n",
    "\n",
    "    demo = gr.TabbedInterface(\n",
    "        [data_retrieving, preprocessing, ml_tools],\n",
    "        [\"Data Retrieving\", \"Preprocessing\", \"Machine Learning Tools\"]\n",
    "    )\n",
    "\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
